{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(a, b):\n",
    "    if (norm(a)*norm(b)) == 0: return 0\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_line = 5\n",
    "N_SEMANTIC_FEATURES = 25\n",
    "semantic_features = {}\n",
    "\n",
    "def dump_mitchell_web_semantic_features(raw_file = os.path.join(\"data\",\"mitchell_semantic_raw.txt\")):\n",
    "    with open(raw_file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        word = None\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # Skip empty\n",
    "            if len(line) >= min_len_line:\n",
    "\n",
    "                # New feature\n",
    "                if \"Features for\" in line:\n",
    "\n",
    "                    # Discard invalid ones (once fully parsed)\n",
    "                    if word and len(semantic_features[word]['features']) < N_SEMANTIC_FEATURES: del semantic_features[word] \n",
    "                        \n",
    "                    word = line.split(\"<a name=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                    semantic_features[word] = { \"features\": [], \"values\": []}\n",
    "\n",
    "                elif word:\n",
    "                    feature_name = line.split(\"(\")[0]\n",
    "                    val = float(line.split(\"(\")[1].split(\")\")[0])\n",
    "                    semantic_features[word][\"features\"].append(feature_name)\n",
    "                    semantic_features[word][\"values\"].append(val)\n",
    "\n",
    "    # Save to file\n",
    "    #with open(os.path.join('data', 'mitchell_semantic_features.json'), 'w') as fp:\n",
    "    #    json.dump(semantic_features, fp)\n",
    "\n",
    "    return semantic_features\n",
    "\n",
    "\n",
    "def load_sorted_semantic_features(file = os.path.join(\"data\",\"mitchell_semantic_features.json\")):\n",
    "    with open(file) as f:\n",
    "        semantic_features = json.load(f)\n",
    "        for word in semantic_features.keys():\n",
    "            # Sort all features\n",
    "            sorted_features = sorted(enumerate(semantic_features[word][\"features\"]), key=lambda x:x[1])\n",
    "            sorted_indices = [i[0] for i in sorted_features]\n",
    "            sorted_values = [semantic_features[word][\"values\"][i] for i in sorted_indices]\n",
    "\n",
    "            # Re-store them\n",
    "            semantic_features[word][\"features\"] = [x[1] for x in sorted_features]\n",
    "            semantic_features[word][\"values\"] = sorted_values\n",
    "            break\n",
    "\n",
    "    return semantic_features\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mitchell_original_data(subject = 1, random_voxels = None):\n",
    "    mdata = scipy.io.loadmat(os.path.join(\"data\", \"mitchell\", f\"data-science-P{subject}.mat\"))\n",
    "    subject_data = {}\n",
    "\n",
    "    # 6 x 60 trials\n",
    "    for i in range(mdata[\"data\"][:].shape[0]):\n",
    "        cond, cond_number, word, word_number, epoch = [x[0] for x in mdata[\"info\"][0][i]]\n",
    "\n",
    "        # Set trial data\n",
    "        if epoch[0] not in subject_data: subject_data[epoch[0]] = {}\n",
    "\n",
    "        if random_voxels:\n",
    "            random_voxels_idx = np.random.choice(mdata[\"data\"][i][0][0].shape[0], random_voxels)\n",
    "            subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0][random_voxels_idx]\n",
    "        else: subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0]\n",
    "\n",
    "    return subject_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_std_voxels(completeFmriData, train_split_indices, K=1000):\n",
    "    n_voxels = completeFmriData[1][\"bell\"].shape[0]\n",
    "    train_words = [k for i,k in enumerate(completeFmriData[1].keys()) if i in train_split_indices] # words used for training\n",
    "    word_wise_activations = np.array([[completeFmriData[epoch][word] for epoch in range(1,7)] for word in train_words]) # (6, 58)    \n",
    "    voxels_stds = np.abs(np.mean(word_wise_activations.std(axis=1), axis=0)) # stds across epochs, mean across words\n",
    "    return np.argpartition(voxels_stds, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitchell_stable_voxels(fmriData, train_split_indices, K = 500):\n",
    "    \n",
    "    # Get total number of voxels\n",
    "    voxels = fmriData[1][\"bell\"].shape[0]\n",
    "\n",
    "    # Get scores of the voxels\n",
    "    scores = []\n",
    "    for vx in range(voxels):\n",
    "\n",
    "        # Gathering epoch-wise brain activity\n",
    "        repetitions = []\n",
    "        for epoch in fmriData.keys():\n",
    "            # store activations only for THAT vx voxel\n",
    "            repetitions.append(np.array([fmriData[epoch][word][vx] for word in fmriData[epoch].keys()]))\n",
    "\n",
    "        # (epochs, words) = (6, 58)\n",
    "        repetitions = np.array(repetitions)\n",
    "\n",
    "        # Compute voxel scores ONLY wrt. the training slice of words\n",
    "        voxel_correlation_score = []\n",
    "        for i in range(repetitions.shape[0]):\n",
    "            for j in range(i+1, repetitions.shape[0]):\n",
    "                # (6, 6) but without triangular down and diagonal = (36 - 6) / 2 = 15 values \n",
    "                voxel_correlation_score.append(np.correlate(repetitions[i, train_split_indices], repetitions[j, train_split_indices]))\n",
    "\n",
    "        voxel_correlation_score = np.array(voxel_correlation_score)\n",
    "        scores.append(np.mean(voxel_correlation_score))\n",
    "    \n",
    "    # indices of the most stable voxels\n",
    "    return np.argpartition(scores, -K)[-K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_best_voxels(scores, K, threshold = 0.2):\n",
    "    scores = np.array(scores)\n",
    "    r2_selected_voxels = np.where(scores > threshold)[0]\n",
    "    return np.array(\n",
    "        sorted( # sort by score, pick first K indices\n",
    "            list(zip(scores[r2_selected_voxels], r2_selected_voxels)), \n",
    "            key = lambda x: x[0]\n",
    "        )\n",
    "    )[:K, 1].astype(np.int32).tolist()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxels_compute_fold_accuracy(predictors, y_pred, y_test, selected_voxels):\n",
    "\n",
    "    true_positives = []\n",
    "    for i, pi in enumerate(y_pred):\n",
    "\n",
    "        pi = pi[selected_voxels]\n",
    "        best_sample_match = np.argmax([cosim(pi, y_test[j, selected_voxels]) for j in range(y_test.shape[0])])\n",
    "    \n",
    "        true_positives.append(int(best_sample_match == i)) # ground truth is aligned with the sample by index, it should match\n",
    "\n",
    "    return np.mean(true_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Loading the dataset\n",
    "epoch = 1\n",
    "K = 500\n",
    "VOXELWISE_ACC_THRESHOLD = 0.2\n",
    "\n",
    "semantic_features = load_sorted_semantic_features()\n",
    "N_words = len(semantic_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Subject 1 ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1770 [00:05<2:56:48,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 25/1770 [01:21<1:32:10,  3.17s/it]"
     ]
    }
   ],
   "source": [
    "subjects = 1\n",
    "accuracies = np.zeros((subjects, 3))\n",
    "\n",
    "for subject in range(1, subjects+1):\n",
    "\n",
    "    print(f\"**** Subject {subject} ****\")\n",
    "\n",
    "    # Subject data\n",
    "    completeFmriData = get_mitchell_original_data(subject=subject)\n",
    "    fmriData = completeFmriData[epoch]\n",
    "    n_voxels = fmriData[\"bell\"].shape[0]\n",
    "\n",
    "    # K-fold cross val\n",
    "    accuracies_r2 = []\n",
    "    accuracies_most_stable = []\n",
    "    accuracies_mitchell = []\n",
    "    \n",
    "    for i in tqdm(range(k_folds)):\n",
    "\n",
    "        # the pair of left out samples rotates\n",
    "        train_indices = np.array(list(range(samples_per_fold * i)) + list(range((samples_per_fold * (i+1)), n_samples)), dtype=np.int32)\n",
    "        test_indices = np.array(list(range((samples_per_fold * i), samples_per_fold * (i + 1))), dtype=np.int32)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        for word in semantic_features.keys():\n",
    "            if word in fmriData.keys():\n",
    "                x = np.array(semantic_features[word][\"values\"])\n",
    "                y = np.array(fmriData[word])\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], Y[train_indices], Y[test_indices]\n",
    "        \n",
    "        # Predicting & scoring\n",
    "        # Lasso is sparsity inducing\n",
    "        predictors = [make_pipeline(StandardScaler(), LinearRegression()) for i in range(n_voxels)]\n",
    "        scores = []\n",
    "\n",
    "        # Fit each voxel predictor\n",
    "        for j, model in enumerate(predictors):\n",
    "            model.fit(X_train, y_train[:, j])\n",
    "            scores.append(model.score(X_test, y_test[:, j]))\n",
    "\n",
    "        # Voxel selections\n",
    "        r2_voxels = r2_best_voxels(scores, K=K, threshold=VOXELWISE_ACC_THRESHOLD)\n",
    "        mitchell_voxels = mitchell_stable_voxels(completeFmriData, train_indices, K=K)\n",
    "        most_stable_voxels = lowest_std_voxels(completeFmriData, train_indices, K=K)\n",
    "\n",
    "        # R2 best voxels\n",
    "        fold_accuracy = voxels_compute_fold_accuracy(predictors, X_test, y_test, r2_voxels)\n",
    "        accuracies_r2.append(fold_accuracy)\n",
    "\n",
    "        # Lowest std.dev voxels\n",
    "        fold_accuracy = voxels_compute_fold_accuracy(predictors, X_test, y_test, most_stable_voxels)\n",
    "        accuracies_most_stable.append(fold_accuracy)\n",
    "\n",
    "        # Mitchell stable voxels\n",
    "        fold_accuracy = voxels_compute_fold_accuracy(predictors, X_test, y_test, mitchell_voxels)\n",
    "        accuracies_mitchell.append(fold_accuracy)\n",
    "\n",
    "    # Subjects accuracies\n",
    "    accuracies[subject-1] = np.array(\n",
    "        [np.mean(accuracies_r2), np.mean(accuracies_most_stable), np.mean(accuracies_mitchell)]\n",
    "    )\n",
    "\n",
    "    with open('accuracies_semantic2fmri.npy', 'wb') as f:\n",
    "        np.save(f, accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2, lowest std, most stable (mitchell)\n",
    "accuracies[:,0].mean(), accuracies[:,1].mean(), accuracies[:,2].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from professor: there is no need to determine the set of best predicted voxels across multiple folds. We just compute the accuracy for each fold and then average."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "In this case fitting is way more expensive, as 21k voxels are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def best_K_predict(X, indices, predictors):\n",
    "    predictors = [predictors[idx] for idx in indices]\n",
    "    y_hat = np.array([predictor.predict(X) for predictor in predictors]) # voxels, sample\n",
    "    return y_hat.reshape(y_hat.shape[1], y_hat.shape[0]) # sample, voxels\n",
    "\n",
    "# voxel_indices\n",
    "\n",
    "y_hat = best_K_predict(X_train, most_stable_voxels, predictors)\n",
    "y = y_train[:, most_stable_voxels]\n",
    "\n",
    "RDM_hat = np.matmul(y_hat, np.matrix.transpose(y_hat))\n",
    "\n",
    "RDM = np.matmul(y, np.matrix.transpose(y))\n",
    "\n",
    "test_pearson = pearsonr(\n",
    "    RDM_hat.flatten(),\n",
    "    RDM.flatten()\n",
    ")\n",
    "\n",
    "print(f\"Test RDMs R^2:\\t{test_pearson}\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Truth\")\n",
    "plt.imshow(RDM)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(RDM_hat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Here the the voxels from the last cross_val iteration have been selected. For these voxels, the object to object distance matrices have similar patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
