{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_line = 5\n",
    "N_SEMANTIC_FEATURES = 25\n",
    "semantic_features = {}\n",
    "\n",
    "def dump_mitchell_web_semantic_features(raw_file = os.path.join(\"data\",\"mitchell_semantic_raw.txt\")):\n",
    "    with open(raw_file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        word = None\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # Skip empty\n",
    "            if len(line) >= min_len_line:\n",
    "\n",
    "                # New feature\n",
    "                if \"Features for\" in line:\n",
    "\n",
    "                    # Discard invalid ones (once fully parsed)\n",
    "                    if word and len(semantic_features[word]['features']) < N_SEMANTIC_FEATURES: del semantic_features[word] \n",
    "                        \n",
    "                    word = line.split(\"<a name=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                    semantic_features[word] = { \"features\": [], \"values\": []}\n",
    "\n",
    "                elif word:\n",
    "                    feature_name = line.split(\"(\")[0]\n",
    "                    val = float(line.split(\"(\")[1].split(\")\")[0])\n",
    "                    semantic_features[word][\"features\"].append(feature_name)\n",
    "                    semantic_features[word][\"values\"].append(val)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join('data', 'mitchell_semantic_features.json'), 'w') as fp:\n",
    "        json.dump(semantic_features, fp)\n",
    "\n",
    "    return semantic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_features = dump_mitchell_web_semantic_features()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mitchell_original_data(subject = 1, random_voxels = None):\n",
    "    mdata = scipy.io.loadmat(os.path.join(\"data\", \"mitchell\", f\"mitchell_subject_{subject}.mat\"))\n",
    "    subject_data = {}\n",
    "\n",
    "    # 6 x 60 trials\n",
    "    for i in range(mdata[\"data\"][:].shape[0]):\n",
    "        cond, cond_number, word, word_number, epoch = [x[0] for x in mdata[\"info\"][0][i]]\n",
    "\n",
    "        # Set trial data\n",
    "        if epoch[0] not in subject_data: subject_data[epoch[0]] = {}\n",
    "\n",
    "        if random_voxels:\n",
    "            random_voxels_idx = np.random.choice(mdata[\"data\"][i][0][0].shape[0], random_voxels)\n",
    "            subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0][random_voxels_idx]\n",
    "        else: subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0]\n",
    "\n",
    "    return subject_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taking the most stable voxels**\n",
    "\n",
    "Supplement online material Mitchell et al., page 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_stable_voxels(fmriData, train_split_indices, K = 500):\n",
    "    \n",
    "    # Get total number of voxels\n",
    "    voxels = fmriData[1][\"bell\"].shape[0]\n",
    "\n",
    "    # Get scores of the voxels\n",
    "    scores = []\n",
    "    for vx in range(voxels):\n",
    "\n",
    "        # Gathering epoch-wise brain activity\n",
    "        repetitions = []\n",
    "        for epoch in fmriData.keys():\n",
    "            # store activations only for THAT vx voxel\n",
    "            repetitions.append(np.array([fmriData[epoch][word][vx] for word in fmriData[epoch].keys()]))\n",
    "\n",
    "        # (epochs, words) = (6, 58)\n",
    "        repetitions = np.array(repetitions)\n",
    "\n",
    "        # Compute voxel scores ONLY wrt. the training slice of words\n",
    "        voxel_correlation_score = []\n",
    "        for i in range(repetitions.shape[0]):\n",
    "            for j in range(i+1, repetitions.shape[0]):\n",
    "                # (6, 6) but without triangular down and diagonal = (36 - 6) / 2 = 15 values \n",
    "                voxel_correlation_score.append(np.correlate(repetitions[i, train_split_indices], repetitions[j, train_split_indices]))\n",
    "\n",
    "        voxel_correlation_score = np.array(voxel_correlation_score)\n",
    "        scores.append(np.mean(voxel_correlation_score))\n",
    "    \n",
    "    # indices of the most stable voxels\n",
    "    return np.argpartition(scores, -K)[-K:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_folds = 30\n",
    "fmriData = get_mitchell_original_data(subject=1)\n",
    "n_samples = len(semantic_features.keys())\n",
    "\n",
    "assert n_samples % k_folds == 0, \"Number of folds must divide the samples in equal parts. Choose a valid multiplier.\"\n",
    "\n",
    "samples_per_fold = (n_samples // k_folds)\n",
    "\n",
    "n_samples, samples_per_fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 1 fold top 50 voxels \n",
      " min: -4.139328861760555\n",
      " mean: -1.106045412417308\n",
      " max: 0.9896598896696216\n",
      "\n",
      "---- 2 fold top 50 voxels \n",
      " min: -7.858748745557463\n",
      " mean: -2.9094566417104306\n",
      " max: 0.9222728071335513\n",
      "\n",
      "---- 3 fold top 50 voxels \n",
      " min: 0.5443939784747384\n",
      " mean: 0.775093892931009\n",
      " max: 0.9998554968393297\n",
      "\n",
      "---- 4 fold top 50 voxels \n",
      " min: -0.11174333043075246\n",
      " mean: 0.4589727537658901\n",
      " max: 0.997823818084851\n",
      "\n",
      "---- 5 fold top 50 voxels \n",
      " min: -1.8887425654771404\n",
      " mean: -0.582562914379422\n",
      " max: 0.9448297236763774\n",
      "\n",
      "---- 6 fold top 50 voxels \n",
      " min: -0.2444070584912783\n",
      " mean: 0.3613918116015352\n",
      " max: 0.9730611753124357\n",
      "\n",
      "---- 7 fold top 50 voxels \n",
      " min: -0.856418369434184\n",
      " mean: 0.04401504271058983\n",
      " max: 0.9503492289929538\n",
      "\n",
      "---- 8 fold top 50 voxels \n",
      " min: -0.23059610179998957\n",
      " mean: 0.3322499449886952\n",
      " max: 0.9836527907931503\n",
      "\n",
      "---- 9 fold top 50 voxels \n",
      " min: 0.007159088992719331\n",
      " mean: 0.3808042757519997\n",
      " max: 0.9995562314281493\n",
      "\n",
      "---- 10 fold top 50 voxels \n",
      " min: -0.0912459502536016\n",
      " mean: 0.18677107327724213\n",
      " max: 0.8879934001051015\n",
      "\n",
      "---- 11 fold top 50 voxels \n",
      " min: 0.3703220642598255\n",
      " mean: 0.6727805369677523\n",
      " max: 0.9519314594330999\n",
      "\n",
      "---- 12 fold top 50 voxels \n",
      " min: 0.38317911690980166\n",
      " mean: 0.6633852906963049\n",
      " max: 0.9943586381228416\n",
      "\n",
      "---- 13 fold top 50 voxels \n",
      " min: 0.13804842577329146\n",
      " mean: 0.521892174637707\n",
      " max: 0.9687407538043017\n",
      "\n",
      "---- 14 fold top 50 voxels \n",
      " min: -0.29804986516252074\n",
      " mean: 0.40067146527640807\n",
      " max: 0.9912254204991708\n",
      "\n",
      "---- 15 fold top 50 voxels \n",
      " min: 0.09395181544550224\n",
      " mean: 0.5121512754761538\n",
      " max: 0.9862341319669256\n",
      "\n",
      "---- 16 fold top 50 voxels \n",
      " min: 0.6707167513820523\n",
      " mean: 0.8451466994858695\n",
      " max: 0.9862638295898549\n",
      "\n",
      "---- 17 fold top 50 voxels \n",
      " min: -0.4394691906380752\n",
      " mean: 0.1138138652276524\n",
      " max: 0.9989765374952754\n",
      "\n",
      "---- 18 fold top 50 voxels \n",
      " min: 0.14012254684195213\n",
      " mean: 0.5469197172841089\n",
      " max: 0.962028834696434\n",
      "\n",
      "---- 19 fold top 50 voxels \n",
      " min: -0.3919447144155026\n",
      " mean: 0.2845042828674727\n",
      " max: 0.9575476239013732\n",
      "\n",
      "---- 20 fold top 50 voxels \n",
      " min: -1.5571101622408108\n",
      " mean: -0.28918008528119254\n",
      " max: 0.981509157015023\n",
      "\n",
      "---- 21 fold top 50 voxels \n",
      " min: -0.37210282574017106\n",
      " mean: 0.18900901651602048\n",
      " max: 0.9846069368835526\n",
      "\n",
      "---- 22 fold top 50 voxels \n",
      " min: 0.2456910237430322\n",
      " mean: 0.5775018238425268\n",
      " max: 0.99144157573399\n",
      "\n",
      "---- 23 fold top 50 voxels \n",
      " min: -0.03073006056319194\n",
      " mean: 0.3357973181642587\n",
      " max: 0.8908676308380681\n",
      "\n",
      "---- 24 fold top 50 voxels \n",
      " min: -0.20785759743935728\n",
      " mean: 0.2649320251897395\n",
      " max: 0.9729571437864653\n",
      "\n",
      "---- 25 fold top 50 voxels \n",
      " min: -0.2050703009251642\n",
      " mean: 0.20097505142306984\n",
      " max: 0.8341976293765507\n",
      "\n",
      "---- 26 fold top 50 voxels \n",
      " min: -0.26804363617036997\n",
      " mean: 0.23907830546729783\n",
      " max: 0.9640351798479034\n",
      "\n",
      "---- 27 fold top 50 voxels \n",
      " min: -0.33359671598436136\n",
      " mean: 0.25891807257466115\n",
      " max: 0.9083653427501406\n",
      "\n",
      "---- 28 fold top 50 voxels \n",
      " min: 0.4167691273406562\n",
      " mean: 0.6602591861321189\n",
      " max: 0.9714453763305329\n",
      "\n",
      "---- 29 fold top 50 voxels \n",
      " min: 0.5484165516522157\n",
      " mean: 0.760826065063647\n",
      " max: 0.9946431359140333\n",
      "\n",
      "---- 30 fold top 50 voxels \n",
      " min: -0.20919130292902088\n",
      " mean: 0.2614722024294852\n",
      " max: 0.9927520641684807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(k_folds):\n",
    "\n",
    "    # Extracting filtered (most stable voxels) training set\n",
    "    train_indices = np.array(list(range(samples_per_fold * i)) + list(range((samples_per_fold * (i+1)), n_samples)), dtype=np.int32)\n",
    "    test_indices = np.array(list(range((samples_per_fold * i), samples_per_fold * (i + 1))), dtype=np.int32)\n",
    "\n",
    "    K = 500\n",
    "    voxels_indices = get_most_stable_voxels(fmriData, train_indices, K = K)\n",
    "\n",
    "    # Filtering the dataset\n",
    "    filteredfMRIData = {}\n",
    "    for epoch in fmriData.keys():\n",
    "        for word, activations in fmriData[epoch].items():\n",
    "            if word not in filteredfMRIData.keys(): filteredfMRIData[word] = []\n",
    "            filteredfMRIData[word].append(activations[voxels_indices])\n",
    "            \n",
    "    for word in filteredfMRIData.keys():\n",
    "        filteredfMRIData[word] = np.mean(filteredfMRIData[word], axis=0)\n",
    "\n",
    "    # Building train set\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for word in semantic_features.keys():\n",
    "        if word in filteredfMRIData.keys():\n",
    "            X.append(semantic_features[word][\"values\"])\n",
    "            Y.append(filteredfMRIData[word])\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], Y[train_indices], Y[test_indices]\n",
    "\n",
    "    # Predicting & scoring\n",
    "    predictors = [LinearRegression() for i in range(K)]\n",
    "    scores = []\n",
    "\n",
    "    # One predictor per voxel\n",
    "    j = 0\n",
    "    for model in predictors:\n",
    "        model.fit(X_train, y_train[:, j])\n",
    "        scores.append(model.score(X_test, y_test[:, j]))\n",
    "        j += 1\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    topK = 50\n",
    "    ind = np.argpartition(scores, -topK)[-topK:]\n",
    "    \n",
    "    print(f\"---- {i+1} fold top {topK} voxels \\n min: {np.min(scores[ind])}\\n mean: {np.mean(scores[ind])}\\n max: {np.max(scores[ind])}\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.array([predictor.predict([X_test[0]])[0] for predictor in predictors])\n",
    "p2 = np.array([predictor.predict([X_test[1]])[0] for predictor in predictors])\n",
    "i1, i2 = y_test # 0, 1\n",
    "\n",
    "print(f\"similarity (p1, i1): {cosim(p1, i1)}\")\n",
    "print(f\"similarity (p1, i2): {cosim(p1, i2)}\")\n",
    "print(f\"similarity (p2, i1): {cosim(p2, i1)}\")\n",
    "print(f\"similarity (p2, i2): {cosim(p2, i2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 80\n",
    "ind = np.argpartition(scores, -K)[-K:]\n",
    "\n",
    "plt.title(\"Semantic features embeddings\")\n",
    "plt.xlabel(\"test accuracy\")\n",
    "plt.ylabel(\"# voxels\")\n",
    "plt.hist(scores[ind], bins=30)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the best 100 predictors per voxel\n",
    "subset_scores = [np.mean(scores[np.argpartition(scores, -subset_size)[-subset_size:]]) for subset_size in range(1, 100)]\n",
    "\n",
    "plt.title(\"top-K voxels accuracy\")\n",
    "plt.plot(subset_scores)\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.grid()\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "K = 20\n",
    "ind = np.argpartition(scores, -K)[-K:]\n",
    "\n",
    "# According to the last subject\n",
    "def best_K_predict(X, indices, predictors):\n",
    "    predictors = [predictors[idx] for idx in indices]\n",
    "    y_hat = np.array([predictor.predict(X) for predictor in predictors]) # voxels, sample\n",
    "    return y_hat.reshape(y_hat.shape[1], y_hat.shape[0]) # sample, voxels\n",
    "\n",
    "y_hat = best_K_predict(X_train, ind, predictors)\n",
    "y = y_train[:, ind]\n",
    "\n",
    "RDM_hat = np.matmul(y_hat, np.matrix.transpose(y_hat))\n",
    "# RDM_hat = (RDM_hat - RDM_hat.min()) / (RDM_hat.max() - RDM_hat.min())\n",
    "\n",
    "RDM = np.matmul(y, np.matrix.transpose(y))\n",
    "# RDM = (RDM - RDM.min()) / (RDM.max() - RDM.min())\n",
    "\n",
    "test_pearson = pearsonr(\n",
    "    RDM_hat.flatten(),\n",
    "    RDM.flatten()\n",
    ")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Truth\")\n",
    "plt.imshow(RDM)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(RDM_hat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
