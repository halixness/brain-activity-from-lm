{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_line = 5\n",
    "N_SEMANTIC_FEATURES = 25\n",
    "semantic_features = {}\n",
    "\n",
    "def dump_mitchell_web_semantic_features(raw_file = os.path.join(\"data\",\"mitchell_semantic_raw.txt\")):\n",
    "    with open(raw_file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        word = None\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # Skip empty\n",
    "            if len(line) >= min_len_line:\n",
    "\n",
    "                # New feature\n",
    "                if \"Features for\" in line:\n",
    "\n",
    "                    # Discard invalid ones (once fully parsed)\n",
    "                    if word and len(semantic_features[word]['features']) < N_SEMANTIC_FEATURES: del semantic_features[word] \n",
    "                        \n",
    "                    word = line.split(\"<a name=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                    semantic_features[word] = { \"features\": [], \"values\": []}\n",
    "\n",
    "                elif word:\n",
    "                    feature_name = line.split(\"(\")[0]\n",
    "                    val = float(line.split(\"(\")[1].split(\")\")[0])\n",
    "                    semantic_features[word][\"features\"].append(feature_name)\n",
    "                    semantic_features[word][\"values\"].append(val)\n",
    "\n",
    "    # Save to file\n",
    "    #with open(os.path.join('data', 'mitchell_semantic_features.json'), 'w') as fp:\n",
    "    #    json.dump(semantic_features, fp)\n",
    "\n",
    "    return semantic_features\n",
    "\n",
    "\n",
    "def load_sorted_semantic_features(file = os.path.join(\"data\",\"mitchell_semantic_features.json\")):\n",
    "    with open(file) as f:\n",
    "        semantic_features = json.load(f)\n",
    "        for word in semantic_features.keys():\n",
    "            # Sort all features\n",
    "            sorted_features = sorted(enumerate(semantic_features[word][\"features\"]), key=lambda x:x[1])\n",
    "            sorted_indices = [i[0] for i in sorted_features]\n",
    "            sorted_values = [semantic_features[word][\"values\"][i] for i in sorted_indices]\n",
    "\n",
    "            # Re-store them\n",
    "            semantic_features[word][\"features\"] = [x[1] for x in sorted_features]\n",
    "            semantic_features[word][\"values\"] = sorted_values\n",
    "            break\n",
    "\n",
    "    return semantic_features\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mitchell_original_data(subject = 1, random_voxels = None):\n",
    "    mdata = scipy.io.loadmat(os.path.join(\"data\", \"mitchell\", f\"data-science-P{subject}.mat\"))\n",
    "    subject_data = {}\n",
    "\n",
    "    # 6 x 60 trials\n",
    "    for i in range(mdata[\"data\"][:].shape[0]):\n",
    "        cond, cond_number, word, word_number, epoch = [x[0] for x in mdata[\"info\"][0][i]]\n",
    "\n",
    "        # Set trial data\n",
    "        if epoch[0] not in subject_data: subject_data[epoch[0]] = {}\n",
    "\n",
    "        if random_voxels:\n",
    "            random_voxels_idx = np.random.choice(mdata[\"data\"][i][0][0].shape[0], random_voxels)\n",
    "            subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0][random_voxels_idx]\n",
    "        else: subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0]\n",
    "\n",
    "    return subject_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_best_voxels(scores, K, threshold = 0.2):\n",
    "    scores = np.array(scores)\n",
    "    r2_selected_voxels = np.where(scores > threshold)[0]\n",
    "    return np.array(\n",
    "        sorted( # sort by score, pick first K indices\n",
    "            list(zip(scores[r2_selected_voxels], r2_selected_voxels)), \n",
    "            key = lambda x: x[0]\n",
    "        )\n",
    "    )[:K, 1].astype(np.int32).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitchell_stable_voxels(voxel_matrices, train_split_indices, K = 500):\n",
    "\n",
    "    # Get scores of the voxels\n",
    "    scores = []\n",
    "    for vx in voxel_matrices:\n",
    "        u, s, vh = np.linalg.svd(vx[:, train_split_indices], full_matrices=True) # SVD, take first eigenvalue\n",
    "        scores.append((s**2)[0])\n",
    "        \n",
    "    # indices of the most stable voxels\n",
    "    return np.argpartition(scores, -K)[-K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voxels_matrices(data):\n",
    "    voxels = data[1][\"bell\"].shape[0]\n",
    "\n",
    "    repetitions = []\n",
    "    for vx in (range(voxels)):\n",
    "        repetitions.append(np.array(\n",
    "            [[data[epoch][word][vx] for word in data[epoch].keys()] for epoch in data.keys()]\n",
    "        ))\n",
    "    return np.array(repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.06 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "completeFmriData = get_mitchell_original_data(subject=1)\n",
    "voxels = get_voxels_matrices(completeFmriData)\n",
    "voxels = mitchell_stable_voxels(voxels, list(range(58)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_2_out_accuracy(y_pred, y_test):\n",
    "\n",
    "    p1, p2 = y_pred\n",
    "    i1, i2 = y_test\n",
    "\n",
    "    pair1_score = cosim(p1, i1) + cosim(p2, i2)\n",
    "    pair2_score = cosim(p1, i2) + cosim(p2, i1)\n",
    "\n",
    "    return int(pair1_score > pair2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "subjects = 9\n",
    "leave2out = LeavePOut(2)\n",
    "K = 500\n",
    "VOXELWISE_ACC_THRESHOLD = 0.2\n",
    "\n",
    "semantic_features = load_sorted_semantic_features()\n",
    "N_words = len(semantic_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros((subjects, 3))\n",
    "\n",
    "for subject in range(1, subjects+1):\n",
    "\n",
    "    print(f\"**** Subject {subject} ****\")\n",
    "\n",
    "    completeFmriData = get_mitchell_original_data(subject=subject)\n",
    "    voxel_matrices = get_voxels_matrices(completeFmriData) # pre-compute voxel 6x58 matrices\n",
    "    \n",
    "    data = completeFmriData[epoch]\n",
    "    n_voxels = data[\"bell\"].shape[0]\n",
    "\n",
    "    # Training\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word in semantic_features.keys():\n",
    "        if word in data.keys():\n",
    "            x = np.array(semantic_features[word][\"values\"])\n",
    "            y = np.array(data[word])\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # leave 2 out cross validation\n",
    "    accuracies_r2 = []\n",
    "    accuracies_most_stable = []\n",
    "    accuracies_mitchell = []\n",
    "    progress_bar = tqdm(range(leave2out.get_n_splits(X)))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(leave2out.split(X)):\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            \n",
    "        # Early voxel selection\n",
    "        # this is the most expensive operation! the overhead lies in fetching\n",
    "        mitchell_voxels = mitchell_stable_voxels(voxel_matrices, train_index, K=K) # extract svds, score voxels, pick best 500\n",
    "        \n",
    "        # Predicting & scoring\n",
    "        predictors = make_pipeline(StandardScaler(), MultiOutputRegressor(LinearRegression(), n_jobs=32))\n",
    "        predictors.fit(X_train, y_train[:, mitchell_voxels])\n",
    "        \n",
    "        # Mitchell stable voxels\n",
    "        y_pred = predictors.predict(X_test)\n",
    "        accuracies_mitchell.append(\n",
    "            leave_2_out_accuracy(y_pred, y_test[:, mitchell_voxels])\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        # R2 best voxels\n",
    "        scores = [r2_score(y_pred[:, i], y_test[:, vx]) for i,vx in enumerate(mitchell_voxels)]\n",
    "        r2_best = r2_best_voxels(scores, K = K)\n",
    "        print(r2_best)\n",
    "        accuracies_r2.append(\n",
    "            leave_2_out_accuracy(y_pred, y_test[:, r2_best])\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "\n",
    "    # Subject mean accuracies\n",
    "    accuracies[subject-1] = np.array(\n",
    "        [np.mean(accuracies_mitchell), 0, 0]\n",
    "    )\n",
    "    print(f\"Accuracy: {np.mean(accuracies_mitchell):.2f}\")\n",
    "    \n",
    "    with open('accuracies_semantic2fmri.npy', 'wb') as f:\n",
    "        np.save(f, accuracies)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| method | subject | voxel selection | accuracy |\n",
    "|---|---|---|---|\n",
    "| multiple regressors | 1 | Most stable | 0.60 |\n",
    "| multiple regressors, no normalization | 1 | Most stable | 0.60 |\n",
    "| Ridge (not voxel-wise), no normalization | 1 | Most stable | 0.45 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from professor: there is no need to determine the set of best predicted voxels across multiple folds. We just compute the accuracy for each fold and then average."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "In this case fitting is way more expensive, as 21k voxels are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def best_K_predict(X, indices, predictors):\n",
    "    predictors = [predictors[idx] for idx in indices]\n",
    "    y_hat = np.array([predictor.predict(X) for predictor in predictors]) # voxels, sample\n",
    "    return y_hat.reshape(y_hat.shape[1], y_hat.shape[0]) # sample, voxels\n",
    "\n",
    "# voxel_indices\n",
    "\n",
    "y_hat = best_K_predict(X_train, mitchell_voxels, predictors)\n",
    "y = y_train[:, mitchell_voxels]\n",
    "\n",
    "RDM_hat = np.matmul(y_hat, np.matrix.transpose(y_hat))\n",
    "\n",
    "RDM = np.matmul(y, np.matrix.transpose(y))\n",
    "\n",
    "test_pearson = pearsonr(\n",
    "    RDM_hat.flatten(),\n",
    "    RDM.flatten()\n",
    ")\n",
    "\n",
    "print(f\"Test RDMs R^2:\\t{test_pearson}\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Truth\")\n",
    "plt.imshow(RDM)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(RDM_hat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Here the the voxels from the last cross_val iteration have been selected. For these voxels, the object to object distance matrices have similar patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
