{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(a, b):\n",
    "    if (norm(a)*norm(b)) == 0: return 0\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(y_hat, y_true, k = 5, THRESHOLD = 0.95):\n",
    "    matched = 0\n",
    "    for i in range(y_hat.shape[0]):\n",
    "        # For each predicted vector => compare true ones\n",
    "        similarities = np.sort(\n",
    "            np.array([cosim(y_hat[i], y_true[j]) for j in range(y_true.shape[0])])\n",
    "        )[::-1]\n",
    "        \n",
    "        has_top_k_match = int(np.sum((similarities[:k] >= THRESHOLD).astype(int)) >= 1)\n",
    "        matched += has_top_k_match\n",
    "\n",
    "    return matched / y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def cross_validate_model(X, Y):\n",
    "\n",
    "    # Shuffle together\n",
    "    temp = list(zip(X, Y))\n",
    "    random.shuffle(temp)\n",
    "    X, Y = zip(*temp)\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "    # Cross validation\n",
    "    crossVal = KFold(n_splits=10)\n",
    "    K = 30\n",
    "    accuracies = []\n",
    "    progress = tqdm(range(crossVal.get_n_splits(X)))\n",
    "\n",
    "    for i, (train_index, test_index) in (enumerate(crossVal.split(X))):\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), PLSRegression(n_components=K))\n",
    "        model.fit(X[train_index], Y[train_index])\n",
    "\n",
    "        # Given a predicted vector, we rank all the 534 vectors in the gold standard data set by decreasing cosine similarity values\n",
    "        y_hat = model.predict(X[test_index]) \n",
    "        accuracies.append(\n",
    "            top_k_accuracy(y_hat, Y)\n",
    "        )\n",
    "        progress.update(1)\n",
    "        \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binder features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"data\", \"binder_semantic_features\", \"word_ratings\", \"WordSet1_Ratings.csv\")\n",
    "\n",
    "binder_features = {}\n",
    "with open(filename, \"r\") as file:\n",
    "    for line in file.readlines()[1:]:\n",
    "        if \",na,\" not in line: # NOTE: incomplete data issue\n",
    "            word = line.split(\",\")[1]\n",
    "            features = [float(x) if x != \"na\" else 0 for x in line.split(\",\")[5:69]]\n",
    "            binder_features[word] = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(binder_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modules.lm import LanguageModel\n",
    "device = \"cuda\"\n",
    "lm = LanguageModel(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GloVe embeddings performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_activations(path, skip_lines=0):\n",
    "    \"\"\"\n",
    "        Returns dataset of fMRI word activations\n",
    "        path            Path to .txt fMRI data vectors (continuous) from Cognival\n",
    "        context_len     Words before the occurring one\n",
    "        data            Returned dictionary with key ['word'] -> {'context', 'activations'}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(path, \"r\") as datafile:\n",
    "        lines = datafile.readlines()[skip_lines:] # skip header\n",
    "        for line in tqdm(lines):\n",
    "            word = line.split(\" \")[0]\n",
    "            activations = np.array([float(x) for x in line.split(\" \")[1:]])\n",
    "            data[word] = {\"activations\": activations}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"data\", \"glove.6B\", f\"glove.6B.100d.txt\")\n",
    "glove_embeddings = get_word_activations(filename, skip_lines = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keys = [k for k in glove_embeddings.keys() if k in binder_features.keys()]\n",
    "len(common_keys) # almost all the binder features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for key in common_keys:\n",
    "    X.append(glove_embeddings[key][\"activations\"])\n",
    "    Y.append(binder_features[key])\n",
    "\n",
    "X = np.array(X).astype(np.float32) # word embeddings\n",
    "Y = np.array(Y).astype(np.float32) # binder features\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"data\", \"experiments\", \"glove_binder_X.npy\"), X)\n",
    "np.save(os.path.join(\"data\", \"experiments\", \"glove_binder_Y.npy\"), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_model(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random embeddings performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for key in binder_features.keys():\n",
    "    Y.append(binder_features[key])\n",
    "\n",
    "Y = np.array(Y).astype(np.float32) # binder features\n",
    "random_X = np.random.rand(Y.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_model(random_X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BERT non-contextual embeddings performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for key in tqdm(binder_features.keys()):\n",
    "    X.append(lm.get_contextualized_word_embedding(key, key))\n",
    "    Y.append(binder_features[key])\n",
    "\n",
    "Y = np.array(Y).astype(np.float32) # binder features\n",
    "X = np.array(X).astype(np.float32) # binder features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"data\", \"experiments\", \"bert_noctx_binder_X.npy\"), X)\n",
    "np.save(os.path.join(\"data\", \"experiments\", \"bert_noctx_binder_Y.npy\"), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_model(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT contextual embeddings performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Because they produce token vectors, following the method proposed by Bommasani, Davis, and Cardie (2020) and Vuli ́ c et al. (2020), we created type representations by randomly sampling 1,000 sentences for each target word from the Wikipedia corpus. We generated a contextualized embedding for each word token by feeding the sentence to the publicly available pre-trained models of ELMo and BERT and taking the token vector of the output layer. Finally, an embedding for each word was obtained by averaging its 1,000 contextualized vectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (C:/Users/xdieg/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905c08b8fc0d42bc9446d3ee0f66defc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wikipedia = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_context_sentences(word, dataset, rnd = 0.5, max_words = 1000, progress = False, patience = 500, max_sentence_len = 200):\n",
    "    \"\"\" Search word in db and grab the context sentence \"\"\"\n",
    "    word_sentences = [] \n",
    "    if progress == True: progress_bar = tqdm(range(max_words))\n",
    "    cnt = 0\n",
    "    word = f\" {word} \"\n",
    "    for row in dataset:\n",
    "        if len(word_sentences) >= max_words: break\n",
    "        if cnt >= patience: break\n",
    "\n",
    "        # Check wikipedia entry containing word with probability 30% (random sampling)\n",
    "        if word in row[\"text\"] and random.uniform(0,1) <= rnd:\n",
    "            cnt = 0\n",
    "            text = row[\"text\"].replace(\"\\n\", \" \")\n",
    "            # Pick all sentences containing that word\n",
    "            for sentence in text.split(\".\"):\n",
    "                if len(word_sentences) >= max_words: break\n",
    "                if word in sentence and len(sentence.split(\" \")) < max_sentence_len:\n",
    "                    word_sentences.append(sentence)\n",
    "                    if progress == True: progress_bar.update(1)\n",
    "        else:\n",
    "            cnt += 1\n",
    "    return word_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wikipedia[\"train\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetching 1000 contexts for each Binder word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 32/434 [03:24<1:08:12, 10.18s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "words = []\n",
    "progress = tqdm(range(len(binder_features.keys())))\n",
    "for i, word in enumerate(binder_features.keys()):\n",
    "    context_sentences = get_context_sentences(word, dataset, progress=False, patience=5e3)\n",
    "    words.append(lm.get_contextualized_embeddings(word, context_sentences, batch_size=4))\n",
    "    progress.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering & visualizing the meanings for a given word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_words = list(zip(list(binder_features.keys())[:38], words[:-1]))\n",
    "len(contextualized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "idx = 21\n",
    "layer = 6\n",
    "word = contextualized_words[idx][0]\n",
    "layer_ctx_word = contextualized_words[idx][1][layer-1].numpy()\n",
    "labels, meaning = lm.get_dominant_meaning(layer_ctx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(layer_ctx_word)\n",
    "\n",
    "dominant_label = None \n",
    "max_l = -1\n",
    "for label in np.unique(labels):\n",
    "    idx = np.where(labels == label)\n",
    "    if len(idx[0]) > max_l:\n",
    "        max_l = len(idx[0])\n",
    "        dominant_label = label\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=label)\n",
    "    \n",
    "plt.title(f\"layer: {layer} / word: '{word}' / # embeds: {layer_ctx_word.shape[0]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(\"data\", \"experiments\", \"bert_binder_X.npy\"), X)\n",
    "# np.save(os.path.join(\"data\", \"experiments\", \"bert_binder_Y.npy\"), Y)\n",
    "# cross_validate_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
