{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_line = 5\n",
    "N_SEMANTIC_FEATURES = 25\n",
    "semantic_features = {}\n",
    "\n",
    "def dump_mitchell_web_semantic_features(raw_file = os.path.join(\"data\",\"mitchell_semantic_raw.txt\")):\n",
    "    with open(raw_file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        word = None\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # Skip empty\n",
    "            if len(line) >= min_len_line:\n",
    "\n",
    "                # New feature\n",
    "                if \"Features for\" in line:\n",
    "\n",
    "                    # Discard invalid ones (once fully parsed)\n",
    "                    if word and len(semantic_features[word]['features']) < N_SEMANTIC_FEATURES: del semantic_features[word] \n",
    "                        \n",
    "                    word = line.split(\"<a name=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                    semantic_features[word] = { \"features\": [], \"values\": []}\n",
    "\n",
    "                elif word:\n",
    "                    feature_name = line.split(\"(\")[0]\n",
    "                    val = float(line.split(\"(\")[1].split(\")\")[0])\n",
    "                    semantic_features[word][\"features\"].append(feature_name)\n",
    "                    semantic_features[word][\"values\"].append(val)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join('data', 'mitchell_semantic_features.json'), 'w') as fp:\n",
    "        json.dump(semantic_features, fp)\n",
    "\n",
    "    return semantic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_features = dump_mitchell_web_semantic_features()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mitchell_original_data(subject = 1, random_voxels = None):\n",
    "    mdata = scipy.io.loadmat(os.path.join(\"data\", \"mitchell\", f\"mitchell_subject_{subject}.mat\"))\n",
    "    subject_data = {}\n",
    "\n",
    "    # 6 x 60 trials\n",
    "    for i in range(mdata[\"data\"][:].shape[0]):\n",
    "        cond, cond_number, word, word_number, epoch = [x[0] for x in mdata[\"info\"][0][i]]\n",
    "\n",
    "        # Set trial data\n",
    "        if epoch[0] not in subject_data: subject_data[epoch[0]] = {}\n",
    "\n",
    "        if random_voxels:\n",
    "            random_voxels_idx = np.random.choice(mdata[\"data\"][i][0][0].shape[0], random_voxels)\n",
    "            subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0][random_voxels_idx]\n",
    "        else: subject_data[epoch[0]][word] = mdata[\"data\"][i][0][0]\n",
    "\n",
    "    return subject_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taking the most stable voxels**\n",
    "\n",
    "Supplement online material Mitchell et al., page 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_stable_voxels(fmriData, train_split_indices, K = 500):\n",
    "    \n",
    "    # Get total number of voxels\n",
    "    voxels = fmriData[1][\"bell\"].shape[0]\n",
    "\n",
    "    # Get scores of the voxels\n",
    "    scores = []\n",
    "    for vx in range(voxels):\n",
    "\n",
    "        # Gathering epoch-wise brain activity\n",
    "        repetitions = []\n",
    "        for epoch in fmriData.keys():\n",
    "            # store activations only for THAT vx voxel\n",
    "            repetitions.append(np.array([fmriData[epoch][word][vx] for word in fmriData[epoch].keys()]))\n",
    "\n",
    "        # (epochs, words) = (6, 58)\n",
    "        repetitions = np.array(repetitions)\n",
    "\n",
    "        # Compute voxel scores ONLY wrt. the training slice of words\n",
    "        voxel_correlation_score = []\n",
    "        for i in range(repetitions.shape[0]):\n",
    "            for j in range(i+1, repetitions.shape[0]):\n",
    "                # (6, 6) but without triangular down and diagonal = (36 - 6) / 2 = 15 values \n",
    "                voxel_correlation_score.append(np.correlate(repetitions[i, train_split_indices], repetitions[j, train_split_indices]))\n",
    "\n",
    "        voxel_correlation_score = np.array(voxel_correlation_score)\n",
    "        scores.append(np.mean(voxel_correlation_score))\n",
    "    \n",
    "    # indices of the most stable voxels\n",
    "    return np.argpartition(scores, -K)[-K:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_folds = 30\n",
    "fmriData = get_mitchell_original_data(subject=1)\n",
    "n_samples = len(semantic_features.keys())\n",
    "\n",
    "assert n_samples % k_folds == 0, \"Number of folds must divide the samples in equal parts. Choose a valid multiplier.\"\n",
    "\n",
    "samples_per_fold = (n_samples // k_folds)\n",
    "\n",
    "n_samples, samples_per_fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 1 fold top 50 voxels \n",
      " min: -4.139328861760621\n",
      " mean: -1.106045412417312\n",
      " max: 0.9896598896696204\n",
      "\n",
      "---- 2 fold top 50 voxels \n",
      " min: -7.858748745557609\n",
      " mean: -2.90945664171041\n",
      " max: 0.922272807133563\n",
      "\n",
      "---- 3 fold top 50 voxels \n",
      " min: 0.5443939784747517\n",
      " mean: 0.7750938929310085\n",
      " max: 0.9998554968393297\n",
      "\n",
      "---- 4 fold top 50 voxels \n",
      " min: -0.11174333043074225\n",
      " mean: 0.4589727537658939\n",
      " max: 0.9978238180848528\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xdieg\\OneDrive\\Desktop\\Study\\Artificial & Biological AI\\seminar_paper\\0_mitchell_study.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m((samples_per_fold \u001b[39m*\u001b[39m i), samples_per_fold \u001b[39m*\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m K \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m voxels_indices \u001b[39m=\u001b[39m get_most_stable_voxels(fmriData, train_indices, K \u001b[39m=\u001b[39;49m K)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Filtering the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m filteredfMRIData \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32mc:\\Users\\xdieg\\OneDrive\\Desktop\\Study\\Artificial & Biological AI\\seminar_paper\\0_mitchell_study.ipynb Cell 13\u001b[0m in \u001b[0;36mget_most_stable_voxels\u001b[1;34m(fmriData, train_split_indices, K)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repetitions\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, repetitions\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m# (6, 6) but without triangular down and diagonal = (36 - 6) / 2 = 15 values \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         voxel_correlation_score\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mcorrelate(repetitions[i, train_split_indices], repetitions[j, train_split_indices]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m voxel_correlation_score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(voxel_correlation_score)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdieg/OneDrive/Desktop/Study/Artificial%20%26%20Biological%20AI/seminar_paper/0_mitchell_study.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(voxel_correlation_score))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_score = []\n",
    "for i in range(k_folds):\n",
    "\n",
    "    # Extracting filtered (most stable voxels) training set\n",
    "    train_indices = np.array(list(range(samples_per_fold * i)) + list(range((samples_per_fold * (i+1)), n_samples)), dtype=np.int32)\n",
    "    test_indices = np.array(list(range((samples_per_fold * i), samples_per_fold * (i + 1))), dtype=np.int32)\n",
    "\n",
    "    K = 500\n",
    "    voxels_indices = get_most_stable_voxels(fmriData, train_indices, K = K)\n",
    "\n",
    "    # Filtering the dataset\n",
    "    filteredfMRIData = {}\n",
    "    for epoch in fmriData.keys():\n",
    "        for word, activations in fmriData[epoch].items():\n",
    "            if word not in filteredfMRIData.keys(): filteredfMRIData[word] = []\n",
    "            filteredfMRIData[word].append(activations[voxels_indices])\n",
    "            \n",
    "    for word in filteredfMRIData.keys():\n",
    "        filteredfMRIData[word] = np.mean(filteredfMRIData[word], axis=0)\n",
    "\n",
    "    # Building train set\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for word in semantic_features.keys():\n",
    "        if word in filteredfMRIData.keys():\n",
    "            X.append(semantic_features[word][\"values\"])\n",
    "            Y.append(filteredfMRIData[word])\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], Y[train_indices], Y[test_indices]\n",
    "    \n",
    "    # Normalization based on train data\n",
    "    normalizer = StandardScaler()\n",
    "    normalizer.fit(X_train)\n",
    "\n",
    "    X_train = normalizer.transform(X_train)\n",
    "    X_test = normalizer.transform(X_test)\n",
    "\n",
    "    # Predicting & scoring\n",
    "    predictors = [LinearRegression() for i in range(K)]\n",
    "    scores = []\n",
    "\n",
    "    # One predictor per voxel\n",
    "    j = 0\n",
    "    for model in predictors:\n",
    "        model.fit(X_train, y_train[:, j])\n",
    "        scores.append(model.score(X_test, y_test[:, j]))\n",
    "        j += 1\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    topK = 50\n",
    "    ind = np.argpartition(scores, -topK)[-topK:]\n",
    "    \n",
    "    print(f\"---- {i+1} fold top {topK} voxels \\n min: {np.min(scores[ind])}\\n mean: {np.mean(scores[ind])}\\n max: {np.max(scores[ind])}\\n\")\n",
    "    avg_score.append(np.mean(scores[ind]))\n",
    "\n",
    "print(f\"Mean of top {topK} scores: {np.mean(avg_score)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.array([predictor.predict([X_test[0]])[0] for predictor in predictors])\n",
    "p2 = np.array([predictor.predict([X_test[1]])[0] for predictor in predictors])\n",
    "i1, i2 = y_test # 0, 1\n",
    "\n",
    "print(f\"similarity (p1, i1): {cosim(p1, i1)}\")\n",
    "print(f\"similarity (p1, i2): {cosim(p1, i2)}\")\n",
    "print(f\"similarity (p2, i1): {cosim(p2, i1)}\")\n",
    "print(f\"similarity (p2, i2): {cosim(p2, i2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "wrt. the last cross validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 80\n",
    "ind = np.argpartition(scores, -K)[-K:]\n",
    "\n",
    "plt.title(\"Semantic features embeddings\")\n",
    "plt.xlabel(\"test accuracy\")\n",
    "plt.ylabel(\"# voxels\")\n",
    "plt.hist(scores[ind], bins=30)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the best 100 predictors per voxel\n",
    "subset_scores = [np.mean(scores[np.argpartition(scores, -subset_size)[-subset_size:]]) for subset_size in range(1, 100)]\n",
    "\n",
    "plt.title(\"top-K voxels accuracy\")\n",
    "plt.plot(subset_scores)\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.grid()\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "K = 20\n",
    "ind = np.argpartition(scores, -K)[-K:]\n",
    "\n",
    "# According to the last subject\n",
    "def best_K_predict(X, indices, predictors):\n",
    "    predictors = [predictors[idx] for idx in indices]\n",
    "    y_hat = np.array([predictor.predict(X) for predictor in predictors]) # voxels, sample\n",
    "    return y_hat.reshape(y_hat.shape[1], y_hat.shape[0]) # sample, voxels\n",
    "\n",
    "y_hat = best_K_predict(X_train, ind, predictors)\n",
    "y = y_train[:, ind]\n",
    "\n",
    "RDM_hat = np.matmul(y_hat, np.matrix.transpose(y_hat))\n",
    "# RDM_hat = (RDM_hat - RDM_hat.min()) / (RDM_hat.max() - RDM_hat.min())\n",
    "\n",
    "RDM = np.matmul(y, np.matrix.transpose(y))\n",
    "# RDM = (RDM - RDM.min()) / (RDM.max() - RDM.min())\n",
    "\n",
    "test_pearson = pearsonr(\n",
    "    RDM_hat.flatten(),\n",
    "    RDM.flatten()\n",
    ")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Truth\")\n",
    "plt.imshow(RDM)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(RDM_hat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
